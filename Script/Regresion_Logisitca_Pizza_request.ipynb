{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Regresion Logisitca Pizza request.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrEdqStf2q_Z",
        "outputId": "223a6d1d-7b51-4f3b-b009-9447a2782a36"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soOxmZvkgmD5"
      },
      "source": [
        "# Directorio donde se encuentra el dataset\n",
        "root_path = '/content/drive/MyDrive/Pizza request/'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDOS0BnHVyZE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aeb9afda-3ba8-40b0-d578-2a95fb500253"
      },
      "source": [
        "# Imports\n",
        "import csv\n",
        "import random\n",
        "import numpy as np\n",
        "import re\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "#import utils \n",
        "#import os\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from gensim.models import Word2Vec, Phrases\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKz-v7mwV2yK"
      },
      "source": [
        "# Lectura del dataset de train\n",
        "trainfile = csv.reader(open(root_path + \"train.csv\"), delimiter='\\t')\n",
        "trainrows = np.array([[c for c in row] for row in trainfile])\n",
        "row_count_train, column_count = np.shape(trainrows)\n",
        "T_train = [int(c) for c in trainrows[:, 0]]\n",
        "P_train = trainrows[:, 1]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdBggoUEZgP-"
      },
      "source": [
        "# Preprocesamiento de los textos\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()       \n",
        "\n",
        "# Eliminación de stop-wrods y stemming de los términos\n",
        "P_train = [re.sub(\"[^a-zA-Z]\", \" \", l.lower()) for l in P_train]\n",
        "P_train = [l.split() for l in P_train] \n",
        "P_train = [[lemmatizer.lemmatize(l) for l in row if l not in stopwords] for row in P_train]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FO3uMg4ke1Pj"
      },
      "source": [
        "# Creación del modelo de word2vec\n",
        "\n",
        "embedding_size = 50\n",
        "model = Word2Vec(sentences = P_train, size=embedding_size, min_count=3, window=5)\n",
        "\n",
        "# Convertir features word2vec\n",
        "\n",
        "vocab = model.wv.vocab\n",
        "keys = list(vocab.keys())\n",
        "filter_unknown = lambda word: vocab.get(word, None) is not None\n",
        "encode = lambda review: list(map(keys.index, filter(filter_unknown, review)))\n",
        "word_vector = list(map(encode, P_train))\n",
        "\n",
        "input_length = embedding_size\n",
        "#input_length = 20\n",
        "#input_length = 150\n",
        "# Convierte en matrix al vector de palabras. Todos tienen la misma longitud\n",
        "X = pad_sequences(sequences=word_vector, maxlen=input_length, padding='post')\n",
        "Y = np.array(T_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2EEGa9N6awD",
        "outputId": "70f2a9ac-2941-44c6-e607-4924bacc52f7"
      },
      "source": [
        "# Veo si hay desbalance de clases\n",
        "zeros = T_train.count(0)\n",
        "ones = T_train.count(1)\n",
        "total = len(T_train)\n",
        "print(zeros,ones, total)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2098 660 2758\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROmbeyyG7swW"
      },
      "source": [
        "# Separacion en train y validacion\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, Y, stratify=Y, test_size=0.33, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67Jd4m9Ej0mj",
        "outputId": "2ab02a87-fafc-4029-c748-044c294951c5"
      },
      "source": [
        "x_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2199,   61,   54, ...,  185,  845,  317],\n",
              "       [1114,  506, 1217, ...,    0,    0,    0],\n",
              "       [ 634, 2156, 3069, ...,    0,    0,    0],\n",
              "       ...,\n",
              "       [ 255,  256,   26, ...,    0,    0,    0],\n",
              "       [ 348,  349,  234, ...,    0,    0,    0],\n",
              "       [ 217,  535,  592, ...,    0,    0,    0]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yy5WQ4j-JOgN",
        "outputId": "fa4f192c-487d-4f9c-aa95-4afa0f48e5a2"
      },
      "source": [
        "unique, counts = np.unique(y_train, return_counts=True)\n",
        "values = dict(zip(unique, counts))\n",
        "\n",
        "# Veo si hay desbalance de clases\n",
        "zeros = values[0]\n",
        "ones = values[1]\n",
        "total = values[0] + values[1]\n",
        "print(zeros,ones, total)\n",
        "\n",
        "# Scaling by total/2 helps keep the loss to a similar magnitude.\n",
        "# The sum of the weights of all examples stays the same.\n",
        "weight_for_0 = (1 / zeros)*(total)/2.0 \n",
        "weight_for_1 = (1 / ones)*(total)/2.0\n",
        "\n",
        "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
        "\n",
        "print('Weight for class 0: {:.2f}'.format(weight_for_0))\n",
        "print('Weight for class 1: {:.2f}'.format(weight_for_1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1405 442 1847\n",
            "Weight for class 0: 0.66\n",
            "Weight for class 1: 2.09\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZrZyiGuY6Lj"
      },
      "source": [
        "## Train y validación"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDMvDUO3-7J9",
        "outputId": "d97a89a3-302b-4d6c-a8be-779a921d4341"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "classifier = LogisticRegression(max_iter = 1000, class_weight = 'balanced')\n",
        "#classifier = LogisticRegression(max_iter = 1000, class_weight = 'class_weight')\n",
        "classifier.fit(x_train, y_train)\n",
        "score = classifier.score(x_test, y_test)\n",
        "\n",
        "print(\"Accuracy:\", score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.6081229418221734\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVRbWiiWaTf7"
      },
      "source": [
        "y_pred = classifier.predict(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jREq9VDQXKAP",
        "outputId": "b3015020-4020-48e8-dee3-18077473a2af"
      },
      "source": [
        "rdo_logistica = f1_score(y_test, y_pred , average=\"macro\")\n",
        "print(f\"f1score: {rdo_logistica}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "f1score: 0.5145293250995266\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-yeA_FJYNo0"
      },
      "source": [
        "## Entrenamiento con todos los datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "on5-KR72YK88",
        "outputId": "3f764513-1f0c-4850-8b65-1614d503c23d"
      },
      "source": [
        "classifier = LogisticRegression(max_iter = 1000, class_weight = 'balanced')\n",
        "classifier.fit(X, Y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
              "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
              "                   max_iter=1000, multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecYKv5syY9NO"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JR0n6wSMYsvL"
      },
      "source": [
        "# Lectura del dataset de test\n",
        "testfile = csv.reader(open(root_path + \"test.csv\"), delimiter='\\t')\n",
        "testrows = np.array([[c for c in row] for row in testfile])\n",
        "row_count_test, column_count = np.shape(testrows)\n",
        "T_test = [int(c) for c in testrows[:, 0]]\n",
        "P_test = testrows[:, 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQ66_ighZpOn"
      },
      "source": [
        "# Eliminación de stop-wrods y stemming de los términos\n",
        "P_test = [re.sub(\"[^a-zA-Z]\", \" \", l.lower()) for l in P_test]\n",
        "P_test = [l.split() for l in P_test] \n",
        "P_test = [[lemmatizer.lemmatize(l) for l in row if l not in stopwords] for row in P_test]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KA34VTuHZ9_w"
      },
      "source": [
        "# Preprocesamiento Test\n",
        "word_vector = list(map(encode, P_test))\n",
        "\n",
        "# Convierte en matrix al vector de palabras. Todos tienen la misma longitud\n",
        "x_test_real = pad_sequences(sequences=word_vector, maxlen=input_length, padding='post')\n",
        "y_test_real = np.array(T_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugVnq8RYaJGj"
      },
      "source": [
        "y_pred_test = classifier.predict(x_test_real)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ZRDYW8yZqOo",
        "outputId": "2330cc6d-1074-4f8e-9ab7-9aef3eb000e0"
      },
      "source": [
        "rdo_logistica_test_macro = f1_score(y_test_real, y_pred_test , average=\"macro\")\n",
        "print(f\"f1score_test_macro: {rdo_logistica_test_macro}\")\n",
        "rdo_logistica_test_micro = f1_score(y_test_real, y_pred_test , average=\"micro\")\n",
        "print(f\"f1score_test_micro: {rdo_logistica_test_micro}\")\n",
        "rdo_logistica_test_weighted = f1_score(y_test_real, y_pred_test , average=\"weighted\")\n",
        "print(f\"f1score_test_weighted: {rdo_logistica_test_weighted}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "f1score_test_macro: 0.5465321262416698\n",
            "f1score_test_micro: 0.6193243734108246\n",
            "f1score_test_weighted: 0.6337112364052722\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}